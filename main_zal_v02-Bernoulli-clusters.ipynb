{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes - A Nonparametric Bayesian Approach to Modeling Overlapping Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal \n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Overlapping Mixture Model with Gaussian clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=0)\n",
    "df = pd.DataFrame(X, columns=['X1', 'X2'])\n",
    "df['y']=y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialiser le centre des clusters (Facultatif)\n",
    "clr = KMeans(n_clusters=3)\n",
    "clr.fit(X,y)\n",
    "clr.score(X)\n",
    "clr.cluster_centers_[1].reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[y==1].plot(x=\"X1\", y=\"X2\", kind=\"scatter\", label=\"1\", color='r', figsize=(10,10))\n",
    "df[y==0].plot(x=\"X1\", y=\"X2\", kind=\"scatter\", label=\"0\",color='green', ax=ax)\n",
    "df[y==2].plot(x=\"X1\", y=\"X2\", kind=\"scatter\", label=\"2\",color='grey', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Je recupere les \"overlapping clusters\" avec les kNearestNeighbor\n",
    "#Si la probailité d'être dans le cluster k est d'au moins 0.1 (à tuner) alors l'observation appartient au cluster k\n",
    "knn =  KNeighborsClassifier(n_neighbors=30)\n",
    "knn.fit(X,y)\n",
    "pred = knn.predict_proba(X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load movie data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Real clusters matrix\n",
    "def possible_clusters(X):\n",
    "    I=X.shape[0]\n",
    "    J=X.shape[1]\n",
    "    result = np.zeros(X.shape)\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            if X[i,j]>0.1:\n",
    "                result[i,j]=1\n",
    "    return result\n",
    "    \n",
    "#Z = pd.get_dummies(y).as_matrix()\n",
    "Z=possible_clusters(pred)\n",
    "\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary data to feed X\n",
    "N=100\n",
    "X=np.random.randint(2,size=N*2).reshape(N,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Figure 3 algorithm\n",
    "###  1. Initialize $\\Theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GAUSSIAN CLUSTERS\n",
    "#mu=multivariate_normal.rvs(mean=np.zeros(2),cov=np.matrix([[1, 0], [0, 1]]),size=3)\n",
    "#sigma=invwishart.rvs(df=4,size=3,scale=np.matrix([[1, 0], [0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MULTIVARIATE BERNOULLI\n",
    "#Initialize Theta matrix K=3 rows, D=2 columns\n",
    "#D=2\n",
    "#N=1000\n",
    "#K=3 fixed for now\n",
    "theta=np.empty([3,2])\n",
    "for i in range(0,3):\n",
    "    for j in range(0,2):\n",
    "        u=np.random.uniform()\n",
    "        theta[i,j]=u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Initialize other elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NumIters = 1\n",
    "N = df.shape[0]\n",
    "Z_hat = np.zeros((N,3)) #Matrix of clusters [observations]*[# clusters - takes 1 if belongs to cluster]\n",
    "PZ_hat = np.zeros((N,3)) #Matrix of cluster probabilities\n",
    "K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On prend les n premieres observations et on leur donne les bons clusters associés pour entrainer le modele\n",
    "n=100\n",
    "for i in range(n):\n",
    "    Z_hat[i,]=Z[i,]\n",
    "Z_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def m_without_i_k(Z, i, k):\n",
    "    result=0\n",
    "    for j in range(Z_hat.shape[0]):\n",
    "        if j!=i:\n",
    "            result+= Z_hat[j,k]\n",
    "    return result\n",
    "\n",
    "def likelihood_bern(X,Z,theta,i,k):  #not normalized\n",
    "    temp=0\n",
    "    lh=0\n",
    "    for d in range(0,2):\n",
    "        temp=temp+Z[i,k]*X[i,d]*np.log(theta[k,d]/(1-theta[k,d]))\n",
    "    lh=np.exp(temp)\n",
    "    return lh\n",
    "\n",
    "\n",
    "for j in range(NumIters):\n",
    "    for i in range(0,N):\n",
    "        k_plus = [] #k+ is the number of clusters which data points, excluding i, belong to\n",
    "        for k_ in range(K):\n",
    "            if Z_hat[i,k_] == 0:\n",
    "                k_plus.append(k_)  #for each obs, if proba to belong to cluster k_ is null, add it to k_plus\n",
    "                                   #for data not in the training set, k_plus will take all possible values in 0...K\n",
    "        print(\"Z initial:\",Z[i,])\n",
    "        print(\"k+=\",k_plus)\n",
    "        for k in k_plus:\n",
    "            if Z_hat[i,k] == 0: #exclude data in the training set for which we already have the true categories\n",
    "                #z_ik ⇠ zik|z−i,k, xi,theta\n",
    "                theta_zi=np.zeros(6).reshape([3,2])\n",
    "                Z_hat[i,k] = 1 #Set Z_hat to one (proposal)\n",
    "                #Compute theta's of the Bernoulli likelihood function -- EQUATION (7) --\n",
    "                for d in range(2):\n",
    "                    num_temp=1\n",
    "                    den_temp1=1\n",
    "                    den_temp2=1\n",
    "                    for k_ in range(K):\n",
    "                        num_temp=num_temp*(theta[k_,d]**Z[i,k_])\n",
    "                        den_temp1=den_temp1*((1-theta[k_,d])**Z[i,k_])\n",
    "                        den_temp2=den_temp2*(theta[k_,d]**Z[i,k_])\n",
    "                    theta_zi[:,d]=num_temp/(den_temp1+den_temp2)\n",
    "                #compute bernouilli likelihood (not normalized)\n",
    "                lh_bern=likelihood_bern(X,Z_hat,theta_zi,i,k)\n",
    "                #compute matrix of probas of Z\n",
    "                PZ_hat[i,k] = (m_without_i_k(Z_hat, i, k)/N)*lh_bern\n",
    "                Z_hat[i,k] = 0  #reset Z_hat to zero\n",
    "        #Propose adding new clusters \n",
    "        #Accept or reject proposal\n",
    "        print(\"Proba Z:\",PZ_hat[i,])\n",
    "        print(\"Z_hat new:\",Z_hat[i,])\n",
    "        for k in k_plus:\n",
    "            if Z_hat[i,k] == 0:\n",
    "                u = np.random.uniform(0,1,1)  #PZ_hat is not normalized to [0,1]\n",
    "                if u[0]<PZ_hat[i,k]:\n",
    "                    Z_hat[i,k]=1\n",
    "                    print(i,k)\n",
    "        print(\"----------------\")\n",
    "    \"\"\"\n",
    "    #Resample theta|Z,X using MH proposal\n",
    "    prob_A=0\n",
    "    omega=0.5\n",
    "    for k in range(K):\n",
    "        for d in range(0,2):\n",
    "            #generate proposal theta'(mu' and sigma') based on Beta(omega*theta,omega*(1-theta))\n",
    "            T_prop=beta.rvs(omega*theta[k,d],omega*(1-theta[k,d]))\n",
    "            T_mh=beta.rvs(omega*T_prop[k,d],omega*(1-T_prop[k,d]))\n",
    "            #likelihood of x_d | ...\n",
    "            lh_mh,lh_prop=np.zeros([2,3])\n",
    "            for k_ in range(K):\n",
    "                lh_mh[k_]=likelihood_bern(X,Z_hat,T_mh,i,k_)\n",
    "                lh_prop[k_]=likelihood_bern(X,Z_hat,T_prop,i,k_)\n",
    "            #priors beta\n",
    "            xxx\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.rvs(omega*theta[k,],omega*(1-theta[k,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get movies data from MovieLens\n",
    "\n",
    "The clusters matrix is Z\n",
    "The binary matrix is X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.read_csv('clusters_matrix.csv', sep=',', index_col=0)\n",
    "X = pd.read_csv('binary_data_matrix.csv', sep=',', index_col=0)\n",
    "#X.columns = X.columns.astype(int)\n",
    "#Z.columns = Z.columns.astype(int)\n",
    "X = X.as_matrix()\n",
    "Z = Z.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_users(data):    \n",
    "    nbr_clear = 0\n",
    "    list_col = []\n",
    "    for i in range(data.shape[1]):\n",
    "        temp_ratings = data[:,i]\n",
    "        nbr_ratings = sum(temp_ratings)    \n",
    "        if nbr_ratings > 20: \n",
    "            list_col.append(i)\n",
    "            #data = np.delete(data, i, 1)\n",
    "        else:\n",
    "            nbr_clear += 1\n",
    "    if len(list_col)>0:\n",
    "        data_cleaned = data[:,list_col]\n",
    "    else:\n",
    "        data_cleaned = data\n",
    "    return data_cleaned, nbr_clear\n",
    "\n",
    "def clear_movies(data):\n",
    "    nbr_clear = 0\n",
    "    list_row = []\n",
    "    for i in range(data.shape[0]):\n",
    "        temp_ratings = data[i,:]\n",
    "        nbr_ratings = sum(temp_ratings)    \n",
    "        if nbr_ratings > 10: \n",
    "            list_row.append(i)\n",
    "            #data = np.delete(data, i, 0)\n",
    "        else:\n",
    "            nbr_clear += 1\n",
    "    if len(list_row)>0:\n",
    "        data_cleaned = data[list_row,:]\n",
    "    else:\n",
    "        data_cleaned = data\n",
    "    return data_cleaned, nbr_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_clear = 1\n",
    "X = X[0:650]\n",
    "\n",
    "while nbr_clear > 0:\n",
    "    X, nbr_clear = clear_movies(X)\n",
    "    X, nbr_clear = clear_users(X)\n",
    "\n",
    "X = X[0:500] \n",
    "Z = Z[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mat_random(mat_x, list_rows):\n",
    "    mat_y = np.zeros((mat_x.shape[0], mat_x.shape[1]))\n",
    "    for i in range(mat_x.shape[0]):\n",
    "        mat_y[i,:] = mat_x[list_rows[i],:]\n",
    "    return mat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to randomize the matrix in order to have efficient train et test datasets\n",
    "import random\n",
    "list_random_rows = random.sample(range(500), 500)\n",
    "X = get_mat_random(X, list_random_rows)\n",
    "Z = get_mat_random(Z, list_random_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of all parameters\n",
    "\n",
    "Our data are a binary matrix : we can use a multivariate Bernouilli simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_without_i_k(Z, i, k):\n",
    "    return (sum(Z[:,k])-Z[i,k])\n",
    "\n",
    "def likelihood_bern(X,Z,theta,i, c):  #not normalized\n",
    "    temp=0\n",
    "    lh=0\n",
    "    for k in range(K):\n",
    "        for d in range(D):\n",
    "            temp=temp+Z[i,k]*X[i,d]*np.log(theta[k,d]/(1-theta[k,d]))\n",
    "            #print('z=', Z[i,k], 'X =', X[i,d], 'log=',np.log(theta[k,d]/(1-theta[k,d]))\n",
    "    lh=np.exp(temp)/c\n",
    "    return lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumIters = 1\n",
    "N = X.shape[0]\n",
    "K = Z.shape[1]\n",
    "D = X.shape[1]\n",
    "\n",
    "Z_hat = np.zeros((N,K))  #Matrix of clusters [observations]*[# clusters - takes 1 if belongs to cluster]\n",
    "PZ_hat = np.zeros((N,K)) #Matrix of cluster probabilities\n",
    "theta = np.empty([K,D])\n",
    "np.random.seed(1234)\n",
    "\n",
    "for i in range(K):\n",
    "    for j in range(D):\n",
    "        u=np.random.beta(alpha/K, 1)\n",
    "        theta[i,j]=u\n",
    "     \n",
    "alpha = round(sum(Z.T).mean()) #alphe controles the expected number of clusters a datapoint will belong to\n",
    "c = sum(likelihood_bern(X, Z, theta, [i for i in range(N)], 1) )\n",
    "\n",
    "#On prend les n premieres observations et on leur donne les bons clusters associés pour entrainer le modele\n",
    "n=200\n",
    "Z_hat[i:n-1,]=Z[i:n-1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for j in range(10):\n",
    "    print(j)\n",
    "    for i in range(n,N,1):\n",
    "        k_plus = [] #k+ is the number of clusters which data points, excluding i, belong to\n",
    "        for k_ in range(K):\n",
    "            if Z_hat[i,k_] == 0:\n",
    "                k_plus.append(k_)  #for each obs, if proba to belong to cluster k_ is null, add it to k_plus\n",
    "                                   #for data not in the training set, k_plus will take all possible values in 0...K\n",
    "\n",
    "        for k in k_plus:\n",
    "            if Z_hat[i,k] == 0: #exclude data in the training set for which we already have the true categories\n",
    "                #z_ik ⇠ zik|z−i,k, xi,theta\n",
    "                Z_hat[i,k] = 1 #Set Z_hat to one (proposal)\n",
    "                #compute bernouilli likelihood (not normalized)\n",
    "                lh_bern=likelihood_bern(X,Z_hat,theta,i,c)\n",
    "                #compute matrix of probas of Z\n",
    "                PZ_hat[i,k] = (m_without_i_k(Z_hat, i, k)/N)*lh_bern\n",
    "                \n",
    "                Z_hat[i,k] = 0  #reset Z_hat to zero\n",
    "                u = np.random.beta(alpha/K, 1)  #PZ_hat is not normalized to [0,1]\n",
    "                if u<PZ_hat[i,k]:\n",
    "                    Z_hat[i,k]=1 \n",
    "                    print('Pz=', PZ_hat[i,k], 'u =', u, 'i=',i, 'k=',k)\n",
    "        #Propose adding new clusters \n",
    "        #Accept or reject proposal\n",
    "        #print(i)\n",
    "    \"\"\"\n",
    "    #Resample theta|Z,X using MH proposal\n",
    "    prob_A=0\n",
    "    omega=0.5\n",
    "    for k in range(K):\n",
    "        for d in range(0,2):\n",
    "            #generate proposal theta'(mu' and sigma') based on Beta(omega*theta,omega*(1-theta))\n",
    "            T_prop=beta.rvs(omega*theta[k,d],omega*(1-theta[k,d]))\n",
    "            T_mh=beta.rvs(omega*T_prop[k,d],omega*(1-T_prop[k,d]))\n",
    "            #likelihood of x_d | ...\n",
    "            lh_mh,lh_prop=np.zeros([2,3])\n",
    "            for k_ in range(K):\n",
    "                lh_mh[k_]=likelihood_bern(X,Z_hat,T_mh,i,k_)\n",
    "                lh_prop[k_]=likelihood_bern(X,Z_hat,T_prop,i,k_)\n",
    "            #priors beta\n",
    "            xxx\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.dot(Z,Z.T)\n",
    "print(U)\n",
    "U_hat = np.dot(Z_hat,Z_hat.T)\n",
    "print(U_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. $U$ and $\\hat{U}$ comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.title.set_text('U')\n",
    "ax2.title.set_text('$\\hat{U}$')\n",
    "\n",
    "ax1.spy(U)\n",
    "ax2.spy(U_hat)\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
